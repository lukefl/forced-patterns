{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["%%<br>\n", "oad packages"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import xarray as xr\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import os\n", "import pickle"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%<br>\n", "USER INPUT"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model = 'NorESM2-LM'\n", "exp = 'ssp370-126aer'\n", "ramipdir = '/gws/nopw/j04/terrafirma/RAMIP/'\n", "# lensdir='/glade/collections/cdg/data/CLIVAR_LE/cesm_lens/Amon/ts/' # directory where surface temperature data are located\n", "ensdir = f\"{ramipdir}{model}{exp}\"\n", "# tsdir = f\"{ramipdir}{model}/{exp}/Amon/ts/\"# directory where surface temperature data are located\n", "# outputdir = '/glade/work/rwills/python_output/forced_component/' # directory for saving Pickle files (copy files and change to a directory you have write access)\n", "outputdir = '/home/users/lfraser/' # directory for saving Pickle files (copy files and change to a directory you have write access)\n", "name = \"cesm_lens\" # name used in Pickle files\n", "varnam = 'ts' # set to ts for full ts field, ts50 for 50\u00b0S to 50\u00b0N \n", "T = np.arange(1920,2006,1/12) # historical simulations start in 1920 for CESM, 1850 for MPI (first input to arrange must match)\n", "# Second input to arrange can be chosen to select period of interest, but change name used in pickle files for end-dates other than 2006"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%<br>\n", "Preprocess SST data (or load from Pickle file)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["MPI-LENS fx file for common analsis grid and land mask<br>\n", "ds_fx = xr.open_dataset(outputdir+'T63GR15_jan_surf.nc')<br>\n", "ds_fx = ds_fx.coarsen(lon=2, lat = 2, boundary='trim').mean()<br>\n", "loni = ds_fx.lon<br>\n", "lati = ds_fx.lat<br>\n", "SLF = ds_fx.SLF<br>\n", "mask = SLF.where(SLF<0.5)+1<br>\n", "mask = np.floor(mask.values)<br>\n", "landmask = SLF.where(SLF>0.5)<br>\n", "landmask = np.ceil(landmask.values)<br>\n", "if varnam == 'ts50':<br>\n", "    mask = mask[8:40,:]<br>\n", "    landmask = landmask[8:40,:]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["try:\n", "    # load pre-processed SST data from Pickle\n", "    ts_all = pickle.load( open(outputdir+name+\"_\"+varnam+\"_all.p\", \"rb\" ))\n", "    ts_clim_all = pickle.load( open(outputdir+name+\"_\"+varnam+\"_clim_all.p\", \"rb\" ))\n", "    lat = ts_all.lat\n", "    lon = ts_all.lon\n", "    time = ts_all.time\n", "        \n", "except:\n", "    # preprocess SST data and save to Pickle\n", "    \n", "    # get data files\n", "    # files = sorted(os.listdir(tsdir))\n", "    members = sorted(os.listdir(ensdir))\n", "    # files = [s for s in files if \"rcp85\" in s]\n", "    n = len(members)\n", "    ne = np.empty(n)\n\n", "    # define axes\n", "    # fname = f\"\n", "    ds0list = []\n", "    for dec in [2015, 2020, 2030, 2040, 2050]:\n", "        deci = dec\n", "        decf = dec + 9 - dec%10\n", "        filename0 = f\"{ensdir}{members[0]}/Amon/tas/gn/v20230810/\n", "            tas_Amon_{model}_{exp}_{members[0]}_gn_{deci}01-{decf}_12.nc\"\n", "        ds0dec = xr.open_dataset(filename0)\n", "        ds0list.append(ds0dec)\n", "    ds0 = xr.concat(ds0list, dim=\"time\")\n", "    lon = ds0.lon\n", "    lat = ds0.lat\n", "    time = ds0.time \n", "    nt = len(time) \n", "    nt_cut = len(T)\n", "    month = np.linspace(1, 12, 12)\n", "    \n", "    # for ii in range(n):\n", "    #     # find ensemble member number\n", "    #     i1 = files[ii].find('85_r')+4\n", "    #     i2 = files[ii].find('i1')\n", "    #     ne[ii] = int(files[ii][i1:i2])\n", "    \n", "    ts_all = np.empty((n,nt_cut,len(lat),len(lon)))\n", "    ts_clim_all = np.empty((n,12,len(lat),len(lon)))\n", "    time = time[0:nt_cut]\n", "    ts_all = xr.DataArray(ts_all, coords=[ne, time, lat, lon], dims=[\"member\", \"time\", \"lat\", \"lon\"])\n", "    ts_clim_all = xr.DataArray(ts_clim_all, coords=[ne, month, lat, lon], dims=[\"member\", \"month\", \"lat\", \"lon\"])\n\n", "    # concatenate all ensemble members into one dataset\n", "    for ii, member in enumerate(members):\n", "        print(ii)\n", "        filename = f\"{ensdir}{member}/Amon/tas/gn/v20230810/\n", "            tas_Amon_{model}_{exp}_{member}_gn_{deci}01-{decf}_12.nc\"\n", "        ds_member = xr.open_dataset(filename)\n", "        # ts = ds_member.tas[-nt:,:,:]\n", "        # ts = ts[0:nt_cut,:,:]\n", "        ts = ds_member.tas[-nt:,:,:]\n", "        ts_clim = ts.groupby('time.month').mean('time')\n", "        ts_anom = ts.groupby('time.month')-ts_clim\n", "        # if ne[ii] >= 35: # workaround because latitude in the CESM runs on a different computer have machine perturbation lat differences\n", "        #     ts_all[ii,:,:,:] = ts_anom.values\n", "        #     ts_clim_all[ii,:,:,:] = ts_clim.values\n", "        # else:\n", "        ts_all[ii,:,:,:] = ts_anom\n", "        ts_clim_all[ii,:,:,:] = ts_clim\n", "        \n", "    # coarsen resolution by a factor of 2\n", "    # ts_all = ts_all.coarsen(lon=2, lat = 2, boundary='trim').mean()\n", "    # ts_clim_all = ts_clim_all.coarsen(lon=2, lat = 2, boundary='trim').mean()\n", "    \n", "    # # interpolate to common analysis grid (land mask not yet applied)\n", "    # if varnam == 'ts50':\n", "    #     lati = lati[8:40] # to exclude latitudes greater than 50 degrees\n", "    # ts_all = ts_all.interp(lon = loni, lat = lati)\n", "    # ts_clim_all = ts_clim_all.interp(lon = loni, lat = lati)\n", "    \n", "    # pickle.dump(ts_all, open(outputdir+name+\"_\"+varnam+\"_all.p\", \"wb\" ),protocol=4)\n", "    # pickle.dump(ts_clim_all, open(outputdir+name+\"_\"+varnam+\"_clim_all.p\", \"wb\" ))\n", "    \n", "ne = ts_all.member\n", "nt = len(ts_all.time)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%<br>\n", "sanity check plot, just shows changes in temperature over the simulation"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["field = ts_all.values\n", "field = np.mean(field,axis=0)\n", "field_diff = np.mean(field[912:1031,:,:],axis=0)-np.mean(field[0:119,:,:],axis=0)\n", "f=plt.figure()\n", "plt.contourf(ts_all.lon.values,ts_all.lat.values,field_diff,np.arange(-1,1.1,0.1),cmap=plt.cm.RdBu_r)\n", "cbar = plt.colorbar()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%<br>\n", "Preprocessing for Large Ensemble EOFs"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["lon = ts_all.lon\n", "lat = ts_all.lat\n", "cosw = np.sqrt(np.cos(lat*np.pi/180))\n", "normvec  = cosw/np.sum(cosw);\n", "scale = np.sqrt(normvec);"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X=ts_all*scale"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_ensmean=X.mean('member')\n", "X_flat = X.stack(index=['time','member']).stack(shape=['lat','lon'])\n", "X_ensmean_flat = X_ensmean.stack(shape=['lat','lon'])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["keep unscaled copies of these variables"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Xt_ensmean=ts_all.mean('member')\n", "Xt_flat = ts_all.stack(index=['time','member']).stack(shape=['lat','lon'])\n", "Xt_ensmean_flat = Xt_ensmean.stack(shape=['lat','lon'])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["index = X_flat.index\n", "n = len(index)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%<br>\n", "Perform ensemble EOF analysis (takes a few minutes), or load from Pickle if it has already been done"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["try:\n", "    # load PCA output from Pickle\n", "    pcvec,evl = pickle.load( open(outputdir+name+\"_\"+varnam+\"_EIG.p\", \"rb\" ))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["except:\n", "    # Large Ensemble EOFs\n", "    Cov = np.matmul(X_flat.values.T,X_flat.values)/(n-1)\n", "    evl,pcvec = np.linalg.eig(Cov)\n", "    pickle.dump([pcvec,evl], open(outputdir+name+\"_\"+varnam+\"_EIG.p\", \"wb\" ),protocol=4)\n", "    \n", "s=np.sqrt(evl)"]}, {"cell_type": "markdown", "metadata": {}, "source": [" keeping the below as a reminder that SVD is much slower than eigenvalue analysis for datasets with long time dimension"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%time<br>\n", "Perform ensemble EOF analysis (SVD takes ~20-25 minutes), or load from Pickle if it has already been done"]}, {"cell_type": "markdown", "metadata": {}, "source": ["ry:<br>\n", "   # load SVD output from Pickle<br>\n", "   u,s = pickle.load( open(outputdir+name+\"_\"+varnam+\"_SVD.p\", \"rb\" ))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["xcept:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    # Large Ensemble EOFs\n", "    \n", "#    #u,s = np.linalg.svd(np.transpose(X_flat.values)/np.sqrt(n-1))\n", "#    u,s = np.linalg.svd(np.transpose(X_flat[0:int(n/10),:].values)/np.sqrt(n/10-1))\n", "#    pickle.dump([u,s], open(outputdir+name+\"_\"+varnam+\"_SVD.p\", \"wb\" ),protocol=4)\n", "    \n", "#eigvals=np.diag(s*s)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%<br>\n", "S/N Maximizing (Forced) Pattern analysis"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["neof=200 # number of EOFs retained in S/N maximizing pattern analysis"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Large Ensemble Forced Patterns"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["S=np.matmul(pcvec[:,0:neof],np.diag(1/s[0:neof]))\n", "Sadj=np.matmul(np.diag(s[0:neof]),pcvec[:,0:neof].T)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ensmeanPCs=np.matmul(X_ensmean_flat.values,S) # ensemble-mean principal components"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["gamma=np.cov(ensmeanPCs.T)  # covariance matrix of ensemble-mean principal components"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["u2,signal_frac,v2=np.linalg.svd(gamma)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["SNP=np.matmul(v2,Sadj)\n", "SNPs_reshaped=SNP.reshape(neof,len(lat),len(lon))/scale.values[None,:,None]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["weights = np.matmul(S,v2.T)\n", "weights = weights.reshape(len(lat),len(lon),neof)*scale.values[:,None,None]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["weights=weights.reshape(len(lat)*len(lon),neof)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["tk = np.matmul(Xt_flat.values,weights) # compute timeseries from full data matrix\n", "tk_emean = np.matmul(Xt_ensmean_flat.values,weights) # compute ensemble-mean timeseries from ensemble-mean data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sign_eof = np.ones((neof,1))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for ii in range(neof):\n", "    if np.mean(SNP[ii,:]) < 0:\n", "        SNPs_reshaped[ii,:,:] = -SNPs_reshaped[ii,:,:]\n", "        SNP[ii,:] = -SNP[ii,:]\n", "        tk[:,ii] = -tk[:,ii]\n", "        tk_emean[:,ii] = -tk_emean[:,ii]\n", "        sign_eof[ii] = -1\n", "        \n", "pickle.dump([tk,tk_emean,SNPs_reshaped,weights,signal_frac], open(outputdir+name+\"_\"+varnam+\"_SNP\"+str(neof)+\".p\", \"wb\" ),protocol=4)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(signal_frac[0:30])\n", "plt.plot(signal_frac,marker='o')\n", "plt.xlim(0,30)\n", "plt.title('Signal Fraction')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["ignal_frac_check = np.zeros(31)<br>\n", "or ii in range(31): <br>\n", "   signal_frac_check[ii] = np.mean(np.square(tk_emean[:,ii]))/np.mean(np.square(tk[:,ii]))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    \n", "#print(signal_frac_check)\n", "#f=plt.figure()\n", "#plt.plot(signal_frac_check,marker='o')\n", "#plt.xlim(0,30)\n", "#plt.title('Signal Fraction Check')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%<br>\n", "Plot S/N maximizing patterns (SNPs)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for neof_plot in range(5): \n", "    f=plt.figure()\n", "    plt.contourf(lon.values,lat.values,np.squeeze(SNPs_reshaped[neof_plot,:,:]),np.arange(-0.6,0.65,0.05),cmap=plt.cm.RdBu_r)\n", "    cbar = plt.colorbar()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%<br>\n", "Plot forced pattern timeseries"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["tk_reshape=tk.reshape(nt,len(ne),neof)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for neof_plot in range(5): \n", "    f=plt.figure()\n", "    [plt.plot(T,tk_reshape[:,mm,neof_plot],color='crimson') for mm in range(40)];\n", "    plt.plot(T,tk_emean[:,neof_plot])\n", "    plt.title('SNP'+str(neof_plot+1))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%<br>\n", "Forced component from leading forced patterns"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["M = 13  # number of forced patterns to retain, choose cutoff based on eigenvalue spectrum, or check significant patterns with bootstrapping"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_forced = np.matmul(tk_emean[:,0:M],SNPs_reshaped[0:M,:,:].reshape(M,len(lat)*len(lon)))\n", "X_forced = X_forced.reshape(len(T),len(lat),len(lon))\n", "# X_forced_land = X_forced*landmask[None,:,:]\n", "X_forced = xr.DataArray(X_forced, coords=[T,lat,lon], dims=[\"time\",\"lat\",\"lon\"])\n", "# X_forced_land = xr.DataArray(X_forced_land, coords=[T,lat,lon], dims=[\"time\",\"lat\",\"lon\"])\n", "# Xt_ensmean_land = Xt_ensmean*landmask"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["GMST_forced = X_forced.mean('lon').mean('lat')\n", "GMST_ensmean = Xt_ensmean.mean('lon').mean('lat')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["rctic_forced = X_forced.sel(lat=slice(90,65)).mean('lon').mean('lat')<br>\n", "rctic_ensmean = Xt_ensmean.sel(lat=slice(90,65)).mean('lon').mean('lat')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["tropical_land_forced = X_forced_land.sel(lat=slice(10,-10)).mean('lon').mean('lat')<br>\n", "tropical_land_ensmean = Xt_ensmean_land.sel(lat=slice(10,-10)).mean('lon').mean('lat')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["US_land_forced = X_forced_land.sel(lon=slice(235,295),lat=slice(45,30)).mean('lon').mean('lat')<br>\n", "US_land_ensmean = Xt_ensmean_land.sel(lon=slice(235,295),lat=slice(45,30)).mean('lon').mean('lat')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Nino34_forced = X_forced.sel(lon=slice(190,240),lat=slice(5,-5)).mean('lon').mean('lat')\n", "Nino34_ensmean = Xt_ensmean.sel(lon=slice(190,240),lat=slice(5,-5)).mean('lon').mean('lat')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["EEP_forced = X_forced.sel(lon=slice(210,270),lat=slice(6,-6)).mean('lon').mean('lat')\n", "EEP_ensmean = Xt_ensmean.sel(lon=slice(210,270),lat=slice(6,-6)).mean('lon').mean('lat')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["WEP_forced = X_forced.sel(lon=slice(120,180),lat=slice(6,-6)).mean('lon').mean('lat')\n", "WEP_ensmean = Xt_ensmean.sel(lon=slice(120,180),lat=slice(6,-6)).mean('lon').mean('lat')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["f=plt.figure()\n", "plt.plot(T,GMST_ensmean)\n", "plt.plot(T,GMST_forced)\n", "plt.title('GMST')\n", "plt.legend(('Ens. Mean','SNP Filtered'))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["f=plt.figure()\n", "plt.plot(T,Nino34_ensmean)\n", "plt.plot(T,Nino34_forced)\n", "plt.title('Nino3.4')\n", "plt.legend(('Ens. Mean','SNP Filtered'))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["f=plt.figure()\n", "plt.plot(T,Nino34_ensmean-GMST_ensmean)\n", "plt.plot(T,Nino34_forced-GMST_forced)\n", "plt.title('Nino34 Anomaly')\n", "plt.legend(('Ens. Mean','SNP Filtered'))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["f=plt.figure()\n", "plt.plot(T,EEP_ensmean-WEP_ensmean)\n", "plt.plot(T,EEP_forced-WEP_forced)\n", "plt.title('Pacific SST Gradient')\n", "plt.legend(('Ens. Mean','SNP Filtered'))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# %%<br>\n", "f=plt.figure()<br>\n", "plt.plot(T,US_land_ensmean)<br>\n", "plt.plot(T,US_land_forced)<br>\n", "plt.title('U.S. Land Surface Temperature')<br>\n", "plt.legend(('Ens. Mean','SNP Filtered'))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# %%<br>\n", "f=plt.figure()<br>\n", "plt.plot(T,tropical_land_ensmean)<br>\n", "plt.plot(T,tropical_land_forced)<br>\n", "plt.title('Tropical Land Surface Temperature')<br>\n", "plt.legend(('Ens. Mean','SNP Filtered'))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}